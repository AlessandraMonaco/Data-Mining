# -*- coding: utf-8 -*-
"""DM_HW2_EX1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBVZXVOAJiks9cdJgppdvCIwo4QkLgYC

# **Importing libraries**
"""

pip install bs4

# for web scraping
import requests # required for HTTP request
import bs4
from bs4 import BeautifulSoup # required for HTML and XML parsing                                                            
import pandas as pd 
import time
import re
import math 
import pickle

# for text processing
import nltk
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn

"""# **Downloading and parsing Amazon pages**"""

#Create lists to store needed info
descriptions=[] # List to store product descriptions
prices=[] # List to store price of the product
prime=[] #Binary list (prime=1, not prime=0)
urls=[] #List to store url of the product page
ratings=[] # List to store ratings of the product

n_pages = 7 # no of pages to scrape in the website

for page in range(1, n_pages+1):
  headers = {"User-Agent":"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 \
  (KHTML, like Gecko) Chrome/41.0.2228.0",} 
  r = requests.get("https://www.amazon.it/s?k=computer&page="+str(page), headers=headers)#,proxies=proxies)
  content = r.content
  soup = BeautifulSoup(content)

  #Parse all items
  for d in soup.findAll('div', attrs={'class':'sg-col-4-of-24 sg-col-4-of-12 \
  sg-col-4-of-36 s-result-item s-asin sg-col-4-of-28 sg-col-4-of-16 sg-col \
  sg-col-4-of-20 sg-col-4-of-32'}): 
    #print(d)
    #this class is the div containing the product name, price, rating...
    description = d.find('span', attrs={'class':'a-size-base-plus a-color-base a-text-normal'})
    price = d.find('span', attrs={'class':'a-offscreen'})
    is_prime = d.find('span', attrs={'class':'aok-relative s-icon-text-medium s-prime'})
    url = d.find('a', attrs={'class':'a-link-normal a-text-normal'})
    rating = d.find('span', attrs={'class':'a-icon-alt'})
    
    #Add items information
    if description is not None:
      descriptions.append(description.text)
      print(description.text)
    else:
      descriptions.append("unknown-description")

    if price is not None:
      prices.append(price.text)
      print(price.text)
    else:
      prices.append('0 â‚¬')

    if is_prime is not None:
      prime.append('1')
      print('1')
    else:
      prime.append('0')
      print('0')

    if url is not None:
      urls.append(url['href'])
      print(url['href'])
    else:
      urls.append("unknown-url")

    if rating is not None:
      score = re.findall("[+-]?\d+\,\d+", rating.text)
      #score = score[0].replace(',','.')
      ratings.append(score[0])
      print(score[0])
    else:
      ratings.append('-1')

   
  time.sleep(100) #stop the code for 100 seconds after each page
  print("\n\n PAGE ", page, " DOWNLOADED! \n\n\n")


#Add products to tsv file
df = pd.DataFrame({'Description':descriptions, 'Price':prices, 'Prime':prime, 'Url':urls, 'Ratings':ratings})
df.to_csv('computer.tsv', index=False, encoding='utf-8', sep = '\t')

"""# **Preprocessing product descriptions**

Let's read and look at our product database:
"""

df = pd.read_csv(r'computer.tsv', sep='\t' )
pd.set_option('display.max_colwidth', -1)

print(df.shape)
df.tail(10)

nltk.download('wordnet')
nltk.download('omw')
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords
set(stopwords.words('italian'))

"""Let's define a function that preprocesses our product description, using nltk library:"""

#this function preprocesses a text (either query or product description document)
def preprocess(text):

  tokens = nltk.word_tokenize(text, language='italian')  #tokenize
  tokens=[token.lower() for token in tokens if token.isalpha() or token.isdigit()] #remove punctuations

  italian_stopwords = stopwords.words('italian')
  tokens = [w for w in tokens if w not in italian_stopwords]  #remove stopwords)

  ita_stemmer = nltk.stem.snowball.ItalianStemmer()
  tokens = [ita_stemmer.stem(w) for w in tokens] #stem
  
  return tokens

corpus = df['Description'].dropna().apply(lambda x: preprocess(x))
corpus.tail()

"""# **Building the inverted index**

After saving the documents in an inverted index, we use vector space model as retrieval model of our search engine.

We need an index such that: term --> (docID, tf-idf(doc, term))


because our algorithm for CosineScore(q) is such that for each QUERY TERM t, we:
- compute tf-idf weight(t,q)
- for each entry of the posting list of term t
-             scores[d]= weight(t,d) weight(t,q)


then normalize score of each document by document lenght

**Constructing the inverted index basing on Term Frequencies**
"""

dictionary = dict()

#Defines Term frequencies in an inverted index structure
def II_Tf(docID,description, dictionary):
  for term in description:
    if term in dictionary:
      if docID not in dictionary[term]:
        dictionary[term][docID] = 1
      else:
        dictionary[term][docID] += 1
    else:
      #Add the term to the vocabulary
      dictionary[term] = {}
      dictionary[term][docID] = 1
  return dictionary

id = 0
for row in corpus:
  #print(row)
  dictionary = II_Tf(id, row, dictionary)
  id += 1
print(dictionary)

"""**Transforming the Tf Inverted Index into a Tf-Idf Inverted Index**"""

#If you want to see the terms in the dictionary, here is the list
#for term in dictionary:
#  print(term)

#If you want to print the postings lists of each term, here is the list
#for term in dictionary:
#  print(term, dictionary[term])

N = corpus.count() #306 docs in total
for term in dictionary:
  df = (len(dictionary[term])) #number of docs containing that term
  #Update Tf as Tf-Idf
  for doc in dictionary[term]:
    #dictionary[term][doc] is the tf
    idf = math.log(N/df)
    dictionary[term][doc] = dictionary[term][doc] * idf

print(dictionary)

"""Save the dictionary in a file (non human readable)"""

ii_file = open("inverted_index.pkl", "wb")
pickle.dump(dictionary, ii_file)
ii_file.close()

"""# **Query Processing and Answering**

First, we read the inverted index stored in the file.
"""

ii_file = open("inverted_index.pkl", "rb")
inverted_index = pickle.load(ii_file)
print(inverted_index)

"""Using this algorithm, we compute cosine scores only for the docs containg at least one term appearing in the query, and not for every document in the corpus. We use the inverted index to find docID containing such query terms (inspecting postings lists of query terms)."""

#This function computes the cosine similarity score between the query and the documents
#containing at least one query term
#It returns the list of docIDs and their scores (but only if score is not 0)
#q is an already preprocessed query

def CosineScore(query):

  #Initialization
  results = []
  scores = [] #empty list of results to be returned (docIDs)
  n_docs = len(df)
  Scores = [0] * n_docs
  Length = [0] * n_docs
  
  for i in range(0,n_docs):
    Length[i]=len(df['Description'][i].split())
  
  #Binary Vectorization for the query => we eliminate duplicate terms
  query = list(dict.fromkeys(query))

  for term in query:
    #fetch correspondent postings list
    for docID in dictionary[term]:
      wtd = dictionary[term][docID]
      Scores[docID] += wtd # *1 because we are using a Binary Vectorization for the query
  
  for d in range(0,n_docs):
    if(Scores[d] != 0):
      Scores[d] = Scores[d] / Length[d] #Normalize scores by lenght
      results.append(d)
      scores.append(Scores[d])
   
  return results, scores



# This function prints the top 10 related products (ordered by descending cosine score)
# the first retrieved product is the MOST RELATED PRODUCT TO THE QUERY
def Answer(q):
  q=preprocess(q) 
  
  results, scores = CosineScore(q)
  res = pd.DataFrame({'DocID':results, 'Scores':scores})

  #sort scores in descending order
  res = res.sort_values(by ='Scores', ascending=False)
  #retrieve just the first 10 items (if you want just THE MOST RELATED, change this line to:
  #res = res[:1] )
  res = res[:10]

  #print all information related to each product
  print("\n\nAVAILABLE PRODUCTS:")
  for id in res['DocID']:
    print(df.iloc[id].to_string(),"\n\n")

"""Let's test the search engine with your query!"""

q = input("Search for a product . . . ")
Answer(q)

"""**Further improvements**: 

- consider also synonyms, for example 'laptop' and 'portatile'

- consider ngrams (ex: '8 gb' instead of '8' and 'gb)

- correct misspellings in the query ...
"""